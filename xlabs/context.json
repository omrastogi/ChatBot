{"contexts": "This paper shows the modeling and performance in deep learning computation for an Assistant Conversational Agent (Chatbot). The utilization of Tensorflow software library, particularly Neural Machine Translation (NMT) model. Acquiring knowledge for modeling is one of the most important task and quite difficult to preprocess it. The Bidirectional Recurrent Neural Networks (BRNN) containing attention layers is used, so that input sentence with large number of tokens (or sentences with more than 20\u201340 words) can be replied with more appropriate conversation. The dataset used in the paper for training of model is used from Reddit. The model is developed to perform English to English translation. The main purpose of this work is to increase the perplexity and learning rate of the model and find Bleu Score for translation in same language. The experiments are conducted using Tensorflow using python 3.6. The perplexity, leaning rate, Bleu score and Average time per 1000 steps are 56.10, 0.0001, 30.16 and 4.5 respectively. One epoch is completed at 23,000 steps. The paper also study MacBook Air as a system for neural network and deep learning. The Chatbot can be defined as a software which help humans to make coherent conversation with machine using natural language like English, etc. The conversation can be engaging at times with large vocabularies and broad range of conversational topics. Recently, the usage of deep learning has increased in industry and Chatbot is one of its application [1], [2], [3]. Fig. 1 shows user using the Chatbot for its various application. This paper will help to create open-domain Chatbot, which can be later subjected to a particular domain, if needed as shown in Fig. 1. It can be done by making changes in dataset, which means training model with particular domain knowledge. Due to open domain nature of the Chatbot, it can be used in making Artificial Intelligence Assistant which can make real life conversation with its user in any topic and situation. To make deep learning utilized by everyone, a major deep learning library Tensorflow is implemented by Google [4] and made available for use as an open source. Tensorflow [5] is Python-friendly library bundled with machine learning and deep learning (neural network) models and algorithms. The paper shows the formation of Chatbot by Neural Machine Translation (NMT) model which is improvement on sequence-to-sequence model. The Chatbot use Bidirectional Recurrent Neural Network (BRNN) [6]. The BRNN was chosen, as conversation or input to the Chatbot is dynamic, which means the length of input is unfixed. The BRNN is also supported by attention mechanism [7], [8], which help to further increase capacity of model to remember longer sequence of sentences. The concept of Bidirectional Recurrent Neural Network, can be understand by taking two independent Recurrent Neural Network (RNN) [9] together, sending signals through their layer in opposite directions. So BRNN can be seen as neural network connecting two hidden layers in opposite directions to a single output. This helps the network to have both forward and backward information at every step, i.e. to receive information from both past and future states. The input is fed in one direction in normal time order, and the other, in reverse order. The concept of Extended Long Short Term Memory (ELSTM) [10] can also be used, with Dependent BRNN (DBRNN), as it help to increase the result by 30% on labeled data. The training of the BRNN is done in a same way as RNN, as two bidirectional neurons do not interact with one another. When forward and backward passes are done [11], then only weights are updated. There have been increase in development of conversational agent systems in commercial market like in retail, banking and education sectors. The research is being done, to improve accuracy and make conversation between Chatbot and user as close to real world conversations. Apart from traditional rule based technique, used earlier in Chatbot development and other straightforward machine learning algorithm, advance concepts and techniques like Natural Language Processing (NLP) techniques and Deep Learning Techniques like Deep Neural Network (DNN) and Deep Reinforcement Learning (DRL) are being used to model and train the Chatbot systems. In early days, the translation was done by breaking up source sentences into multiple token or chunks and then translated phrase-by-phrase. Sequence-to-Sequence has been a popular model based on neural network and deep learning, used in Neural Machine Translation [13]. It is used in tasks like machine translation, speech recognition, and text summarization. Sequence-to-Sequence model has an encoder and a decoder, both having Vanilla RNN [14] by default. The encoder take source sentence as input to build a thought vector. A thought vector is a vector space containing sequence of numbers which represent meaning of the sentence. Then finally, decoder process the thought vector fed to it and emit a translation called a target sequence or sentence. But Vanilla RNN fails when long sequence of sentences are fed to model, as information needs to be remembered. This information frequently becomes larger for bigger datasets and create bottleneck for RNN networks. The variation in RNN has to be used like BRNN with Attention mechanism or Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) [15] to handle failure in longer sequences. To understand meaning behind the sentence, the intention, facts and emotions described in the sentence, it must be analyzed. In [3], the statistical difference was 63.33% to identify two groups or personalities on basis of their sentiments. The technique of NLP and machine learning helps to deeply analyze the sentence sentiments and make comfortable environment for humans to make conversation with machine. If dataset is in text, sentiment features can be classified as text level or document level [16]. The deep learning methods like Convolutional Neural Network (CNN) [17] and RNN are used in document sentiment classification, as it is the difficult one out of the two above mentioned classification. Also developing Chatbot for young adolescent, engaging them in their most preferred channel of communication, that are smartphones and successfully helping them to adult focused care. In [2], the engagement time was 97%. The Kyoto-NMT is an open-source implementation of NMT paradigm [18]. It used chained, a Deep learning Framework. It has used two layer of LSTM with Attention model in its Recurrent Neural Network. It has also used whitespaces for making token in data preparation. The vector size used by it is of 1000. The training data is a sentence-aligned parallel corpus that is in utf-8 text files: one for source language sentences and the other for target language sentences. In this, a JSON file is created, containing all the parameters. SQLite is used for database creation and for keeping track of training progress. The bleu score is computed by applying greedy search on validation dataset. As the real translation was there (Japanese-to-English), the bleu score they reached is of 26.22. The Snowbot is a Chatbot, developed on Tensorflow and MXNet [19]. The Chatbot uses Cornell movie Dialog Corpus with 221,282 QA, size 22\u202fMB and twitter chat with 377,265 QA having emoji in sentences, size 51\u202fMB. The Chatbot was developed in desktop having NVIDIA GTX 1080 8\u202fGB and 5\u202fGB RAM. The neural network contains two layer of LSTM with 256 hidden units. The vocabulary size was of 50000. The model completed 7 epochs in 1\u202fh. The perplexity was 90 and 135 for Cornell and Twitter dataset respectively. Currently, there are many performance metrics, and certain measurement standards are followed across industry for Chatbot [20]. Different organizations need Chatbot according to the nature of their work and market surrounding it. One of the most important performance metric for Chatbot is the structure and the length of its conversation. The length of output sentence must be appropriate and in context to the conversation being done. Shorter and simple the structure of sentence in output, faster the solution, does increase the customer satisfaction rate. To understand this metric with an example of banking sector, in banks, the Chatbot is mainly used to guide the user through the bank\u2019s policy, schemes and other customer inquiries about their account. This serve user to perform their tasks quicker, and also lower the human call assistance, thus cutting cost in the service. The consumer satisfaction also bring second metric-the retention rate, very important for Chabot success. The companies aim for significantly high retention rate, indicating customer satisfaction. The automated calls and Chatbot messengers are being used to replace other communication mediums (i.e. lowering call volumes by humans). The retention rate increases when Chatbot are more trained to support the user in managing their account without speaking to a human assistant. Another metric is the ability of the Chatbot, to produce the personalized reply to the user. This means that the Chatbot should take in the source sentence, understand and analyze it, and produce an output statement mapped to the particular problem or query of the user. The companies try to personalize and customize the output statement according to each user\u2019s need, like bank suggesting user a relevant offer or credit card scheme according to the balance in the user\u2019s account, salary, current loan and its spending history. For example, Erica, which is a Chatbot, is Bank of America\u2019s AI virtual assistant, which combine predictive analysis and NLP, to help its users to access their balance information, tips on better saving the money depending upon their spending habits, making transaction between accounts and schedule meetings at financial centers and banks. The e-market and Retail Chatbots make engaging environment for users to shop. Through their environment, the Chabot transform itself in a personal assistant for assisting in shopping. Unlike banking and financial sector Chatbots, Retail conversational agents are designed to look for higher number of conversational steps by holding the users attention, providing details and encouraging them to browse more and ultimately purchase the product. For instance, Ebay\u2019s ShopBot, help users to find best deals from its list of billion products. It is easy to-talk-to, like a friend, either if one is searching for a specific product or browsing to find something new. The above discussed studies shows network designed for small sized datasets and for short input sentences which are not fit for real life conversation as human tends to speak in longer sentences. To counter real world conversation, model like BRNN is important to know conversation context and references, from past as well as future. Attention mechanism is important attachment to the network as it help to weigh the particular references from the input sentences. Also for evaluating Chatbot performances there are no hard metrics [21], but parameters like Perplexity, Learning rate and Bleu score can represent as to how close one can approach at the time of training the model. The model, BRNN with Attention model not only help in short but also in longer tokens. Attention model help us to remember longer sequences at a time and also help in context problem where both historical and future information is required. In real world as language may not necessarily be in perfect sequence, sometimes one has to use context, hear full conversation before going back and responding to words leading up to that point. Human tends to speak in longer sentences to understand the meaning. This is the reason that makes combination of BRNN and Attention model, perfectly right choice for Chatbots. The activation function in neural network are the function attached to the node (or neuron), which get triggered when the input value in the current node is relevant in making prediction. There are many types of activation functions, but with BRNN, sigmoid and logistic activation functions are mostly used. For example, in the network shown in Fig. 3, an input of couple of statements has been given. Statement 1- He said, \u201cIndira went to the market.\u201d and statement 2- He said, \u201cIndira Gandhi was a great Prime Minister.\u201d At an instance, when statement 2 was inserted into the network, at time step 3, where word \u201cIndira\u201d was inserted, Y3\u02c6 cell prediction or output signal is checked, and due to bidirectional in nature, the forward information flows from cell 1st and 2nd to 3rd cell, as well as, backward flow of information from 9th cell (through all cells in between) to 3rd, help the cell to predict that \u2018Indira\u2019 in statement 2 is Prime Minister and not \u2018Indira\u2019 who went to the market, in statement 1. If simple RNN have been used, the output must be according to statement 1, as the network didn\u2019t have the future information. This is because the RNN are unidirectional, i.e. with positive direction of time [22]. In attention mechanism, attention vector is generated by calculating score and then calculated vector is retained in memory, so as to choose between best candidate vectors. The score is calculated by comparing each hidden target with source hidden state. For applying attention mechanism, single directional RNN can be used above BRNN structure. Each cell in the Attention network is given context, as an input. This type of network is also called context-aware attention network [23]. The predicted value from each BRNN cell is taken, combined with value from the previous state (or neuron) of the attention network for calculating the attention value. One can also say that context is weight of the features from different timestamp. The Reddit dataset [29] has been used to make database for the Chatbot. The dataset contain comments of January, year 2015. The format of data is in JSON format. The content of dataset is parent_id, comment_body, score, subreddit, etc. The score is most useful to set the acceptable data criteria as this show that this particular comment is most accurate reply to the parent_comment or parent_body. Subreddit can be used to make some specific type of Chatbot like scientific or other particular domain Bots. A subreddit is a specific online community, and the posts associated with it are dedicated to a particular topic that people write about. The database formed after pre-processing the dataset have size of 2.42\u202fGB. The database contain 10,935,217 rows (i.e. number of parent comment-reply comment pairs). For training after creation of database, rows have to be divided into training data and test data. For both, two files are created (i.e., Parent comment and Reply comment). Training data contains 3,027,254 pairs and Test data contains 5100 pairs. There are also list of protected phrases (e.g.www.xyz.com should be a single token) and blacklisted words, to avoid feeding it to learning network. The training files are fed to multiprocessing tokenizer, as they are CPU intensive. The sentences will be divided into tokens on basis of space and punctuation. Each token will act as vocabulary. For each step, vocabulary size is 15000. The size is appropriate for systems having virtual memory of 4 Gigabytes. The RegX module is used for formulating search pattern for vocabulary. It is faster than standard library and it is basically used to check whether a string contain a specific search pattern. The neural network must be designed as mentioned above. Once the training starts, the main concerned hyperparameters (HParams) in metrics are bleu score (bleu), perplexity (ppl) and learning rate (Lr). Bleu score tells, how good the model is translating a sentence from one language to another language. It should be as high as possible. Perplexity is a measure of the probability distribution, or it tells about model prediction error. Learning rate reflects the model\u2019s learning progress in the network. As in this paper, language at both ends of the model is English, so Perplexity is more useful than bleu score. Learning rate is useful but only when model is trained with large data and for longer period of time. If model is trained for limited period of time or with less data, no significant change in learning rate will be observed.    "}